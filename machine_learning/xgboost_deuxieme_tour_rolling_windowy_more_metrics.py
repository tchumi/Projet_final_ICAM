import pandas as pd
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
from scipy.spatial.distance import cosine
from scipy.stats import entropy

# ðŸ“‚ Chargement des donnÃ©es
data_path = "C:/Users/Admin.local/Documents/Projet_final_data/Piketty_data/elections_socio_fusionnes.csv"
output_report = "C:/Users/Admin.local/Documents/Projet_final_data/Piketty_data/prediction_rolling_window_XGBOOST_deuxieme_tour.txt"

df = pd.read_csv(data_path, dtype={"codecommune": str, "annÃ©e": int})

# ðŸ“Œ SÃ©lection des features pour le 2áµ‰ tour (avec rÃ©sultats du 1áµ‰Ê³ tour)
features_2nd_tour = [
    'exprimesT2', 
    'revratio', 'pchom', 'pouvr', 'pcadr', 'pibratio',
    # Ajout des rÃ©sultats du 1áµ‰Ê³ tour
    'pvoteG', 'pvoteCG', 'pvoteC', 'pvoteCD', 'pvoteD', 'pvoteTG', 'pvoteTD',
    'pvoteTGratio', 'pvoteTDratio'
]
target_columns = ['pvoteT2_ED', 'pvoteT2_D', 'pvoteT2_CD', 'pvoteT2_C', 'pvoteT2_G']

# ðŸ“Œ DÃ©finition des pÃ©riodes d'entraÃ®nement et de test
years_train = [2002, 2007, 2012, 2017]
test_years = [2007, 2012, 2017, 2022]

# ðŸ“Œ Fonction d'Ã©valuation des mÃ©triques
def compute_metrics(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred, multioutput='uniform_average')
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)

    # Conversion en distributions de probabilitÃ©s pour Cosine Similarity et KL Divergence
    y_true_prob = y_true / np.sum(y_true, axis=1, keepdims=True)
    y_pred_prob = y_pred / np.sum(y_pred, axis=1, keepdims=True)

    cosine_sim = np.mean([1 - cosine(y_true_prob[i], y_pred_prob[i]) for i in range(len(y_true))])
    kl_div = np.mean([entropy(y_true_prob[i], y_pred_prob[i]) for i in range(len(y_true))])

    return {"MAE": mae, "RMSE": rmse, "RÂ²": r2, "Cosine Sim": cosine_sim, "KL Divergence": kl_div}

# ðŸ“Œ Fonction d'entraÃ®nement et validation Rolling Window pour le 2áµ‰ tour
def rolling_window_xgboost(df, features, target_columns, years_train, test_years):
    results = []

    for train_end, test_year in zip(years_train, test_years):
        print(f"\nðŸ”„ **Rolling Window : Train jusqu'Ã  {train_end}, Test sur {test_year}**")

        # DÃ©finition des jeux de donnÃ©es
        train = df[df["annÃ©e"] <= train_end]
        test = df[df["annÃ©e"] == test_year]

        X_train, y_train = train[features], train[target_columns]
        X_test, y_test = test[features], test[target_columns]

        # ðŸ“Œ DÃ©finition et entraÃ®nement du modÃ¨le XGBoost
        model = xgb.XGBRegressor(objective="reg:squarederror", n_estimators=200, learning_rate=0.05, max_depth=8, reg_lambda=1)
        model.fit(X_train, y_train)

        # ðŸ“Œ PrÃ©dictions et Ã©valuation
        y_pred = model.predict(X_test)
        metrics = compute_metrics(y_test.values, y_pred)

        print(f"ðŸ“‰ MAE : {metrics['MAE']:.4f}, RMSE : {metrics['RMSE']:.4f}, RÂ² : {metrics['RÂ²']:.4f}")
        print(f"ðŸ” Cosine Similarity : {metrics['Cosine Sim']:.4f}, KL Divergence : {metrics['KL Divergence']:.4f}")

        results.append({
            "train_fin": train_end, "test": test_year,
            **metrics
        })

    return pd.DataFrame(results)

# ðŸ“Œ ExÃ©cution du Rolling Window
df_results = rolling_window_xgboost(df, features_2nd_tour, target_columns, years_train, test_years)

# ðŸ“Š GÃ©nÃ©ration du rapport
report = "\nðŸ“Š **RÃ©sultats Rolling Window - XGBoost DeuxiÃ¨me Tour (avec votes 1áµ‰Ê³ tour)** ðŸ“Š\n" + "-"*50 + "\n"
for _, row in df_results.iterrows():
    report += f"\nðŸ”„ **Train jusqu'Ã  {row['train_fin']}, Test sur {row['test']}**\n"
    report += f"ðŸ“‰ MAE : {row['MAE']:.4f}\n"
    report += f"ðŸ“‰ RMSE : {row['RMSE']:.4f}\n"
    report += f"ðŸ“Š RÂ² : {row['RÂ²']:.4f}\n"
    report += f"ðŸ” Cosine Similarity : {row['Cosine Sim']:.4f}\n"
    report += f"ðŸ” KL Divergence : {row['KL Divergence']:.4f}\n"

# ðŸ“‚ Sauvegarde du rapport
with open(output_report, "w", encoding="utf-8") as f:
    f.write(report)

print(report)
print(f"âœ… Rapport sauvegardÃ© dans : {output_report}")
